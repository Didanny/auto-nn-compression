{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Install dependencies\n"
      ],
      "metadata": {
        "id": "uwCXBRDJUR5-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx\n",
        "!pip install onnxruntime\n",
        "\n",
        "import torch.onnx\n",
        "import onnx\n",
        "import onnxruntime"
      ],
      "metadata": {
        "id": "FEYQU3LrUVpS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1429c59-b6b7-4963-bc27-0cc64dd1eb65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnx\n",
            "  Downloading onnx-1.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from onnx) (1.22.4)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.10/dist-packages (from onnx) (4.7.1)\n",
            "Installing collected packages: onnx\n",
            "Successfully installed onnx-1.14.0\n",
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting coloredlogs (from onnxruntime)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (23.5.26)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (23.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (1.11.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Installing collected packages: humanfriendly, coloredlogs, onnxruntime\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-1.15.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "import copy\n",
        "import time\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch.nn.utils.prune as prune\n",
        "from torchsummary import summary\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIob0ZxSQ18L",
        "outputId": "f4889a2c-e936-44ff-b769-e7003f0e3c55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(path):\n",
        "    model = models.resnet18()\n",
        "    model.fc = nn.Linear(512, 100)\n",
        "    model.load_state_dict(torch.load(path, map_location=device))\n",
        "    model.to(device)\n",
        "    return model\n",
        "\n",
        "\n",
        "model = load_model(path = '/content/drive/MyDrive/UCI/ResNet18_CIFAR100.pth')\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gG4NOhoMQe4Q",
        "outputId": "22c3fb5a-33c3-4921-aed7-7cd8affe37f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=100, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Export onnx file"
      ],
      "metadata": {
        "id": "91x1260ngwFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "model.to(device)\n",
        "\n",
        "input_data = torch.randn(1, 3, 32, 32).to(device)  # model input_size\n",
        "\n",
        "# export model to '.onnx' format\n",
        "torch.onnx.export(model, input_data, \"output2.onnx\", operator_export_type=torch.onnx.OperatorExportTypes.ONNX_ATEN_FALLBACK)\n",
        "\n",
        "print(\"Successful\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7b0RoAiQCfV",
        "outputId": "b5c6a3a1-08e7-433e-c91e-4a88017e3ebb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============= Diagnostic Run torch.onnx.export version 2.0.1+cu118 =============\n",
            "verbose: False, log level: Level.ERROR\n",
            "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
            "\n",
            "Successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load 'onnx' file"
      ],
      "metadata": {
        "id": "u9kheGi14-1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the onnx format file\n",
        "onnx_model = onnx.load('/content/drive/MyDrive/UCI/output2.onnx')  # from model_compression.ipynb"
      ],
      "metadata": {
        "id": "zz_5BrLbq5f_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test for 'onnx' format file\n",
        "\n",
        "onnx_model.graph.node[0].input\n",
        "onnx_model.graph.node[0].output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mr7PJVdN6OYr",
        "outputId": "fb7c6c28-ab65-4dfb-d927-a77783a48cc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/conv1/Conv_output_0']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building Graph"
      ],
      "metadata": {
        "id": "kfAEtt4us0KX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Node class definition\n",
        "class Node:\n",
        "    def __init__(self, name, node_type, inputs, outputs):\n",
        "        self.name = name  # str\n",
        "        self.node_type = node_type  # str\n",
        "        self.inputs = inputs  # List[str]\n",
        "        self.outputs = outputs  # List[str]\n",
        "        self.parent = []  # parent node names\n",
        "        self.child_nodes = []  # child node names\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"node_type={self.node_type}, parent={', '.join(self.parent)}, child_nodes={', '.join(self.child_nodes)}\"\n"
      ],
      "metadata": {
        "id": "KZmDRNQVs1Lc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "node_dict = {}\n",
        "\n",
        "# Create all the nodes from onnx file\n",
        "for node in onnx_model.graph.node:\n",
        "    node_name = node.name\n",
        "    node_type = node.op_type\n",
        "    node_inputs = list(node.input)\n",
        "    node_outputs = list(node.output)\n",
        "\n",
        "    node_dict[node_name] = Node(node_name, node_type, node_inputs, node_outputs)  # create Node instance\n",
        "\n",
        "# Find dependencies each other\n",
        "for node in onnx_model.graph.node:\n",
        "    current_node = node_dict[node.name]\n",
        "    # Check one's output with other's input\n",
        "    for output_name in node.output:\n",
        "        for another_node in onnx_model.graph.node:\n",
        "            if output_name in another_node.input:\n",
        "                # If matched\n",
        "                if another_node.name in node_dict:\n",
        "                    # append to the list\n",
        "                    current_node.child_nodes.append(another_node.name)  # for child nodes\n",
        "                    node_dict[another_node.name].parent.append(current_node.name)  # for parent nodes\n",
        "\n",
        "node_dict  # key : node_name / value : Node instance"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvEfRSOfAmgO",
        "outputId": "c274ae64-e60c-4840-abfc-c82ada29ba17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'/conv1/Conv': node_type=Conv, parent=, child_nodes=/relu/Relu,\n",
              " '/relu/Relu': node_type=Relu, parent=/conv1/Conv, child_nodes=/maxpool/MaxPool,\n",
              " '/maxpool/MaxPool': node_type=MaxPool, parent=/relu/Relu, child_nodes=/layer1/layer1.0/conv1/Conv, /layer1/layer1.0/Add,\n",
              " '/layer1/layer1.0/conv1/Conv': node_type=Conv, parent=/maxpool/MaxPool, child_nodes=/layer1/layer1.0/relu/Relu,\n",
              " '/layer1/layer1.0/relu/Relu': node_type=Relu, parent=/layer1/layer1.0/conv1/Conv, child_nodes=/layer1/layer1.0/conv2/Conv,\n",
              " '/layer1/layer1.0/conv2/Conv': node_type=Conv, parent=/layer1/layer1.0/relu/Relu, child_nodes=/layer1/layer1.0/Add,\n",
              " '/layer1/layer1.0/Add': node_type=Add, parent=/maxpool/MaxPool, /layer1/layer1.0/conv2/Conv, child_nodes=/layer1/layer1.0/relu_1/Relu,\n",
              " '/layer1/layer1.0/relu_1/Relu': node_type=Relu, parent=/layer1/layer1.0/Add, child_nodes=/layer1/layer1.1/conv1/Conv, /layer1/layer1.1/Add,\n",
              " '/layer1/layer1.1/conv1/Conv': node_type=Conv, parent=/layer1/layer1.0/relu_1/Relu, child_nodes=/layer1/layer1.1/relu/Relu,\n",
              " '/layer1/layer1.1/relu/Relu': node_type=Relu, parent=/layer1/layer1.1/conv1/Conv, child_nodes=/layer1/layer1.1/conv2/Conv,\n",
              " '/layer1/layer1.1/conv2/Conv': node_type=Conv, parent=/layer1/layer1.1/relu/Relu, child_nodes=/layer1/layer1.1/Add,\n",
              " '/layer1/layer1.1/Add': node_type=Add, parent=/layer1/layer1.0/relu_1/Relu, /layer1/layer1.1/conv2/Conv, child_nodes=/layer1/layer1.1/relu_1/Relu,\n",
              " '/layer1/layer1.1/relu_1/Relu': node_type=Relu, parent=/layer1/layer1.1/Add, child_nodes=/layer2/layer2.0/conv1/Conv, /layer2/layer2.0/downsample/downsample.0/Conv,\n",
              " '/layer2/layer2.0/conv1/Conv': node_type=Conv, parent=/layer1/layer1.1/relu_1/Relu, child_nodes=/layer2/layer2.0/relu/Relu,\n",
              " '/layer2/layer2.0/relu/Relu': node_type=Relu, parent=/layer2/layer2.0/conv1/Conv, child_nodes=/layer2/layer2.0/conv2/Conv,\n",
              " '/layer2/layer2.0/conv2/Conv': node_type=Conv, parent=/layer2/layer2.0/relu/Relu, child_nodes=/layer2/layer2.0/Add,\n",
              " '/layer2/layer2.0/downsample/downsample.0/Conv': node_type=Conv, parent=/layer1/layer1.1/relu_1/Relu, child_nodes=/layer2/layer2.0/Add,\n",
              " '/layer2/layer2.0/Add': node_type=Add, parent=/layer2/layer2.0/conv2/Conv, /layer2/layer2.0/downsample/downsample.0/Conv, child_nodes=/layer2/layer2.0/relu_1/Relu,\n",
              " '/layer2/layer2.0/relu_1/Relu': node_type=Relu, parent=/layer2/layer2.0/Add, child_nodes=/layer2/layer2.1/conv1/Conv, /layer2/layer2.1/Add,\n",
              " '/layer2/layer2.1/conv1/Conv': node_type=Conv, parent=/layer2/layer2.0/relu_1/Relu, child_nodes=/layer2/layer2.1/relu/Relu,\n",
              " '/layer2/layer2.1/relu/Relu': node_type=Relu, parent=/layer2/layer2.1/conv1/Conv, child_nodes=/layer2/layer2.1/conv2/Conv,\n",
              " '/layer2/layer2.1/conv2/Conv': node_type=Conv, parent=/layer2/layer2.1/relu/Relu, child_nodes=/layer2/layer2.1/Add,\n",
              " '/layer2/layer2.1/Add': node_type=Add, parent=/layer2/layer2.0/relu_1/Relu, /layer2/layer2.1/conv2/Conv, child_nodes=/layer2/layer2.1/relu_1/Relu,\n",
              " '/layer2/layer2.1/relu_1/Relu': node_type=Relu, parent=/layer2/layer2.1/Add, child_nodes=/layer3/layer3.0/conv1/Conv, /layer3/layer3.0/downsample/downsample.0/Conv,\n",
              " '/layer3/layer3.0/conv1/Conv': node_type=Conv, parent=/layer2/layer2.1/relu_1/Relu, child_nodes=/layer3/layer3.0/relu/Relu,\n",
              " '/layer3/layer3.0/relu/Relu': node_type=Relu, parent=/layer3/layer3.0/conv1/Conv, child_nodes=/layer3/layer3.0/conv2/Conv,\n",
              " '/layer3/layer3.0/conv2/Conv': node_type=Conv, parent=/layer3/layer3.0/relu/Relu, child_nodes=/layer3/layer3.0/Add,\n",
              " '/layer3/layer3.0/downsample/downsample.0/Conv': node_type=Conv, parent=/layer2/layer2.1/relu_1/Relu, child_nodes=/layer3/layer3.0/Add,\n",
              " '/layer3/layer3.0/Add': node_type=Add, parent=/layer3/layer3.0/conv2/Conv, /layer3/layer3.0/downsample/downsample.0/Conv, child_nodes=/layer3/layer3.0/relu_1/Relu,\n",
              " '/layer3/layer3.0/relu_1/Relu': node_type=Relu, parent=/layer3/layer3.0/Add, child_nodes=/layer3/layer3.1/conv1/Conv, /layer3/layer3.1/Add,\n",
              " '/layer3/layer3.1/conv1/Conv': node_type=Conv, parent=/layer3/layer3.0/relu_1/Relu, child_nodes=/layer3/layer3.1/relu/Relu,\n",
              " '/layer3/layer3.1/relu/Relu': node_type=Relu, parent=/layer3/layer3.1/conv1/Conv, child_nodes=/layer3/layer3.1/conv2/Conv,\n",
              " '/layer3/layer3.1/conv2/Conv': node_type=Conv, parent=/layer3/layer3.1/relu/Relu, child_nodes=/layer3/layer3.1/Add,\n",
              " '/layer3/layer3.1/Add': node_type=Add, parent=/layer3/layer3.0/relu_1/Relu, /layer3/layer3.1/conv2/Conv, child_nodes=/layer3/layer3.1/relu_1/Relu,\n",
              " '/layer3/layer3.1/relu_1/Relu': node_type=Relu, parent=/layer3/layer3.1/Add, child_nodes=/layer4/layer4.0/conv1/Conv, /layer4/layer4.0/downsample/downsample.0/Conv,\n",
              " '/layer4/layer4.0/conv1/Conv': node_type=Conv, parent=/layer3/layer3.1/relu_1/Relu, child_nodes=/layer4/layer4.0/relu/Relu,\n",
              " '/layer4/layer4.0/relu/Relu': node_type=Relu, parent=/layer4/layer4.0/conv1/Conv, child_nodes=/layer4/layer4.0/conv2/Conv,\n",
              " '/layer4/layer4.0/conv2/Conv': node_type=Conv, parent=/layer4/layer4.0/relu/Relu, child_nodes=/layer4/layer4.0/Add,\n",
              " '/layer4/layer4.0/downsample/downsample.0/Conv': node_type=Conv, parent=/layer3/layer3.1/relu_1/Relu, child_nodes=/layer4/layer4.0/Add,\n",
              " '/layer4/layer4.0/Add': node_type=Add, parent=/layer4/layer4.0/conv2/Conv, /layer4/layer4.0/downsample/downsample.0/Conv, child_nodes=/layer4/layer4.0/relu_1/Relu,\n",
              " '/layer4/layer4.0/relu_1/Relu': node_type=Relu, parent=/layer4/layer4.0/Add, child_nodes=/layer4/layer4.1/conv1/Conv, /layer4/layer4.1/Add,\n",
              " '/layer4/layer4.1/conv1/Conv': node_type=Conv, parent=/layer4/layer4.0/relu_1/Relu, child_nodes=/layer4/layer4.1/relu/Relu,\n",
              " '/layer4/layer4.1/relu/Relu': node_type=Relu, parent=/layer4/layer4.1/conv1/Conv, child_nodes=/layer4/layer4.1/conv2/Conv,\n",
              " '/layer4/layer4.1/conv2/Conv': node_type=Conv, parent=/layer4/layer4.1/relu/Relu, child_nodes=/layer4/layer4.1/Add,\n",
              " '/layer4/layer4.1/Add': node_type=Add, parent=/layer4/layer4.0/relu_1/Relu, /layer4/layer4.1/conv2/Conv, child_nodes=/layer4/layer4.1/relu_1/Relu,\n",
              " '/layer4/layer4.1/relu_1/Relu': node_type=Relu, parent=/layer4/layer4.1/Add, child_nodes=/avgpool/GlobalAveragePool,\n",
              " '/avgpool/GlobalAveragePool': node_type=GlobalAveragePool, parent=/layer4/layer4.1/relu_1/Relu, child_nodes=/Flatten,\n",
              " '/Flatten': node_type=Flatten, parent=/avgpool/GlobalAveragePool, child_nodes=/fc/Gemm,\n",
              " '/fc/Gemm': node_type=Gemm, parent=/Flatten, child_nodes=}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just test"
      ],
      "metadata": {
        "id": "8Q8sqcFag1xZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "att = ['name', 'node_type', 'inputs', 'outputs', 'parent', 'child_nodes']  # define in the Node class\n",
        "\n",
        "# test for Conv0\n",
        "for node in att:\n",
        "    print(f\"{node}: {getattr(node_dict[onnx_model.graph.node[0].name], node)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbmVDYn9AhMI",
        "outputId": "c8e86c23-3ed4-4523-886d-cd0d98321047"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name: /conv1/Conv\n",
            "node_type: Conv\n",
            "inputs: ['input.1', 'onnx::Conv_193', 'onnx::Conv_194']\n",
            "outputs: ['/conv1/Conv_output_0']\n",
            "parent: []\n",
            "child_nodes: ['/relu/Relu']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Node class definition\n",
        "# class Node:\n",
        "#     def __init__(self, name, node_type):\n",
        "#         self.name = name  # str\n",
        "#         self.node_type = node_type  # str\n",
        "#         self.parent = [] # parent node, list of nodes if there are multiple inputs\n",
        "#         self.child_nodes = []\n",
        "\n",
        "#     def __repr__(self):\n",
        "#         return f\"node_type={self.node_type}, parent={', '.join([node.name for node in self.parent])}, child_nodes={', '.join([node.name for node in self.child_nodes])}\"\n",
        "\n",
        "# node_dict = {}  # key : node_name\n",
        "\n",
        "# # First, create all the nodes\n",
        "# for node in onnx_model.graph.node:\n",
        "#     node_name = node.output[0]  # use the first output name as the node name\n",
        "#     node_type = node.op_type  # str\n",
        "#     node_dict[node_name] = Node(node_name, node_type)\n",
        "\n",
        "# # Next, link the nodes together\n",
        "# for node in onnx_model.graph.node:\n",
        "#     current_node = node_dict[node.output[0]]  # use the first output name as the node name\n",
        "#     for input_name in node.input:\n",
        "#         if input_name in node_dict:\n",
        "#             parent_node = node_dict[input_name]\n",
        "#             parent_node.child_nodes.append(current_node)\n",
        "#             current_node.parent.append(parent_node)\n",
        "\n",
        "# # print node_dict\n",
        "# node_dict\n"
      ],
      "metadata": {
        "id": "cXfNyDYs9GLy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}